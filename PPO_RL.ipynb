{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPgpkFCKKhlX",
    "outputId": "3c2c8bf6-e702-429e-8235-8e00622ae400"
   },
   "source": [
    "!pip install gym[atari]\n",
    "!pip install autorom[accept-rom-license]\n",
    "!pip install highway-env"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKyhtgVRKrup",
    "outputId": "5c02d69c-0acc-4c5e-8200-54bdbd7e86ac",
    "ExecuteTime": {
     "end_time": "2024-12-10T18:00:41.026865Z",
     "start_time": "2024-12-10T18:00:39.493383Z"
    }
   },
   "source": [
    "import highway_env\n",
    "import gymnasium as gym\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import os\n",
    "import cv2\n",
    "import torch.distributions as distributions\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical\n",
    "import pickle"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_jcGMZIMUc80",
    "ExecuteTime": {
     "end_time": "2024-12-10T18:00:41.037761Z",
     "start_time": "2024-12-10T18:00:41.034255Z"
    }
   },
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.state_values[:]\n",
    "        del self.is_terminals[:]\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ILhP1EthUdF8",
    "ExecuteTime": {
     "end_time": "2024-12-10T18:00:45.318525Z",
     "start_time": "2024-12-10T18:00:45.313147Z"
    }
   },
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_std_init=0.99):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        self.action_var = torch.full((action_dim,), action_std_init * action_std_init)\n",
    "\n",
    "        # actor\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "                        nn.Linear(state_dim, 512),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(256, action_dim),\n",
    "                        nn.Tanh()\n",
    "                        )\n",
    "\n",
    "\n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "                        nn.Linear(state_dim, 512),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(256, 1)\n",
    "                    )\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std)\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "\n",
    "\n",
    "        action_mean = self.actor(state)\n",
    "        cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        state_val = self.critic(state)\n",
    "\n",
    "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
    "\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "\n",
    "        action_mean = self.actor(state)\n",
    "        action_var = self.action_var.expand_as(action_mean)\n",
    "        cov_mat = torch.diag_embed(action_var)\n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "\n",
    "        return action_logprobs, state_values, dist_entropy\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3glGjf0M3INJ",
    "ExecuteTime": {
     "end_time": "2024-12-10T18:00:45.831998Z",
     "start_time": "2024-12-10T18:00:45.825291Z"
    }
   },
   "source": [
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, action_std_init=0.99):\n",
    "\n",
    "\n",
    "        self.action_std = action_std_init\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, action_std_init)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "                    ])\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, action_std_init)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "\n",
    "        self.action_std = new_action_std\n",
    "        self.policy.set_action_std(new_action_std)\n",
    "        self.policy_old.set_action_std(new_action_std)\n",
    "\n",
    "\n",
    "\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "\n",
    "\n",
    "            self.action_std = self.action_std - action_std_decay_rate\n",
    "            self.action_std = round(self.action_std, 4)\n",
    "            if (self.action_std <= min_action_std):\n",
    "                self.action_std = min_action_std\n",
    "\n",
    "            self.set_action_std(self.action_std)\n",
    "\n",
    "\n",
    "\n",
    "    def select_action(self, state):\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state)\n",
    "                action, action_logprob, state_val = self.policy_old.act(state)\n",
    "\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "            self.buffer.state_values.append(state_val)\n",
    "\n",
    "            return action.detach().cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach()\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach()\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach()\n",
    "        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach()\n",
    "\n",
    "        # calculate advantages\n",
    "        advantages = rewards.detach() - old_state_values.detach()\n",
    "        total_loss=0\n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "\n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
    "\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss+=loss\n",
    "\n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "        return total_loss"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "aTj-t-N55QUl",
    "outputId": "e4a52330-be82-4d08-a9b1-ff6455afe75b"
   },
   "source": [
    "\n",
    "action_std = None\n",
    "\n",
    "rewards_list=[]\n",
    "rendered_frames=[]\n",
    "losses_list=[]\n",
    "\n",
    "K_epochs = 40               # update policy for K epochs\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.99                # discount factor\n",
    "\n",
    "lr_actor = 0.0003       # learning rate for actor network\n",
    "lr_critic = 0.001       # learning rate for critic network\n",
    "\n",
    "\n",
    "env = gym.make(\"parking-v0\", render_mode=\"rgb_array\")\n",
    "env.unwrapped.config['add_walls']=False\n",
    "env.unwrapped.config['duration']=40\n",
    "\n",
    "# state space dimension\n",
    "state_dim = 6\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "\n",
    "# initialize a PPO agent\n",
    "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip)\n",
    "\n",
    "lengths_list=[]\n",
    "rewards_list=[]\n",
    "losses_list=[]\n",
    "rendered_frames=[]\n",
    "time_step = 0\n",
    "i_episode = 0\n",
    "\n",
    "max_episodes=5000\n",
    "pbar = tqdm.trange(max_episodes)\n",
    "loss= 0.0\n",
    "flag=0\n",
    "\n",
    "for episode in pbar:\n",
    "    state,_ = env.reset(seed=0)\n",
    "    state=state['observation']\n",
    "    current_ep_reward = 0\n",
    "    step=0\n",
    "\n",
    "    # for t in range(1, max_ep_len+1):\n",
    "    for _ in range(10000):\n",
    "        # select action with policy\n",
    "        action = ppo_agent.select_action(state)\n",
    "        state, reward, done, terminated ,_ = env.step(action)\n",
    "        state=state['observation']\n",
    "        # saving reward and is_terminals\n",
    "        ppo_agent.buffer.rewards.append(reward)\n",
    "        ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "        time_step +=1\n",
    "        current_ep_reward += reward\n",
    "\n",
    "        step+=1\n",
    "        time_step +=1\n",
    "        # update PPO agent\n",
    "        if time_step % 1000 == 0:\n",
    "            loss =ppo_agent.update()\n",
    "            flag=1\n",
    "\n",
    "        # if continuous action space; then decay action std of ouput action distribution\n",
    "        if time_step % 1000 == 0:\n",
    "            ppo_agent.decay_action_std(0.98, 0.1)\n",
    "\n",
    "        if episode % 499==0:\n",
    "          rendered_frames.append(env.render())\n",
    "\n",
    "        if done or terminated:\n",
    "            break\n",
    "    # loss=ppo_agent.update()\n",
    "    if flag==1:\n",
    "      if type(loss) != float:\n",
    "        loss=loss.clone().detach().numpy().sum()\n",
    "        flag=0\n",
    "    losses_list.append(loss)\n",
    "    lengths_list.append(step)\n",
    "    rewards_list.append(current_ep_reward)\n",
    "    pbar.set_description(\n",
    "                      f'Episode: {episode} | Steps: {step + 1} | Return: {current_ep_reward:5.2f} |Loss: {loss}| STD: { ppo_agent.action_std} '\n",
    "            )\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "extlYsjgjAKg",
    "ExecuteTime": {
     "end_time": "2024-12-10T20:13:47.268690Z",
     "start_time": "2024-12-10T20:13:47.266035Z"
    }
   },
   "source": [
    "def moving_average(data, *, window_size = 50):\n",
    "    \"\"\"Smooths 1-D data array using a moving average.\n",
    "\n",
    "    Args:\n",
    "        data: 1-D numpy.array\n",
    "        window_size: Size of the smoothing window\n",
    "\n",
    "    Returns:\n",
    "        smooth_data: A 1-d numpy.array with the same size as data\n",
    "    \"\"\"\n",
    "    # assert data.ndim == 1\n",
    "    kernel = np.ones(window_size)\n",
    "    smooth_data = np.convolve(data, kernel) / np.convolve(\n",
    "        np.ones_like(data), kernel\n",
    "    )\n",
    "    return smooth_data[: -window_size + 1]"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "STb9Yzv-jtH1",
    "ExecuteTime": {
     "end_time": "2024-12-10T20:14:50.044536Z",
     "start_time": "2024-12-10T20:14:50.041949Z"
    }
   },
   "source": [
    "file_path_PPO_returns = \"PPO_Returns.pkl\"\n",
    "file_path_PPO_losses=\"PPO_Losses.pkl\"\n",
    "file_path_PPO_lengths=\"PPO_Lengths.pkl\"\n",
    "file_path_PPO_frames=\"PPO_frames.pkl\""
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4ts2Cbivj0lr",
    "ExecuteTime": {
     "end_time": "2024-12-10T20:14:55.115287Z",
     "start_time": "2024-12-10T20:14:52.729212Z"
    }
   },
   "source": [
    "with open(file_path_PPO_returns, 'wb') as f:\n",
    "    pickle.dump(rewards_list, f)\n",
    "with open(file_path_PPO_losses, 'wb') as f:\n",
    "    pickle.dump(losses_list, f)\n",
    "with open(file_path_PPO_frames, 'wb') as f:\n",
    "    pickle.dump(rendered_frames, f)\n",
    "with open(file_path_PPO_lengths, 'wb') as f:\n",
    "    pickle.dump(lengths_list, f)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4NIcTF9xj3XS",
    "ExecuteTime": {
     "end_time": "2024-12-10T20:14:56.930711Z",
     "start_time": "2024-12-10T20:14:56.593602Z"
    }
   },
   "source": [
    "with open(file_path_PPO_returns, 'rb') as f :\n",
    "  PPO_returns = pickle.load(f)\n",
    "with open(file_path_PPO_losses, 'rb') as f :\n",
    "  PPO_losses = pickle.load(f)\n",
    "with open(file_path_PPO_frames, 'rb') as f :\n",
    "  PPO_frames = pickle.load(f)\n",
    "with open(file_path_PPO_lengths, 'rb') as f :\n",
    "  PPO_lengths = pickle.load(f)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3rahWue6j8XV"
   },
   "source": [
    "# YOUR PLOTTING CODE HERE\n",
    "plt.figure(figsize=(10, 6))\n",
    "# plt.subplot(3, 1, 1)\n",
    "plt.plot(PPO_returns, label='Returns (Raw Data)', alpha=0.5)\n",
    "plt.plot(moving_average(PPO_returns), label='Returns (Moving Average)', color='orange')\n",
    "plt.title('Returns')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Return')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(PPO_lengths, label='Lengths (Raw Data)', alpha=0.5)\n",
    "plt.plot(moving_average(PPO_lengths), label='Lengths (Moving Average)', color='orange')\n",
    "plt.title('Lengths')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Length')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(PPO_losses, label='Losses (Raw Data)')\n",
    "plt.plot(moving_average(PPO_losses), label='Losses (Moving Average)', color='orange')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
