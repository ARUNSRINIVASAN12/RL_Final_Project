{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8aKDneFgIOnC",
    "outputId": "8149b5fa-1a61-46eb-bff2-972674136c05"
   },
   "source": [
    "!pip install gym[atari]\n",
    "!pip install autorom[accept-rom-license]\n",
    "!pip install highway-env"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "import highway_env\n",
    "import gymnasium as gym\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import os\n",
    "import cv2\n",
    "import torch.distributions as distributions\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tT-HF9v1IRMS",
    "outputId": "e334c06c-bee5-43b7-ca27-ed6b81baaaff"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the Actor network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))  # Outputs action in the range [-1, 1]\n",
    "        return x\n",
    "\n",
    "# Define the Critic network\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class SAC:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256, alpha=0.2, gamma=0.99, tau=0.005):\n",
    "        self.actor = Actor(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "        self.critic1 = Critic(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.critic1_target = Critic(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.critic1_target.load_state_dict(self.critic1.state_dict())\n",
    "\n",
    "        self.critic2 = Critic(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.critic2_target = Critic(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.critic2_target.load_state_dict(self.critic2.state_dict())\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=3e-4)\n",
    "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=3e-4)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action = self.actor(state).cpu().data.numpy()\n",
    "        return action\n",
    "\n",
    "    def update(self, replay_buffer, batch_size=64):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = replay_buffer.sample(batch_size)\n",
    "        state_batch = torch.FloatTensor(state_batch).to(device)\n",
    "        action_batch = torch.FloatTensor(action_batch).to(device)\n",
    "        reward_batch = torch.FloatTensor(reward_batch).unsqueeze(1).to(device)\n",
    "        next_state_batch = torch.FloatTensor(next_state_batch).to(device)\n",
    "        done_batch = torch.FloatTensor(1 - done_batch).unsqueeze(1).to(device)\n",
    "\n",
    "        # Update critic networks\n",
    "        with torch.no_grad():\n",
    "            next_action = self.actor_target(next_state_batch)\n",
    "            next_q1 = self.critic1_target(next_state_batch, next_action)\n",
    "            next_q2 = self.critic2_target(next_state_batch, next_action)\n",
    "            next_q_target = torch.min(next_q1, next_q2)\n",
    "            target_q = reward_batch + self.gamma * next_q_target * done_batch\n",
    "\n",
    "        current_q1 = self.critic1(state_batch, action_batch)\n",
    "        current_q2 = self.critic2(state_batch, action_batch)\n",
    "\n",
    "        critic1_loss = F.mse_loss(current_q1, target_q)\n",
    "        critic2_loss = F.mse_loss(current_q2, target_q)\n",
    "\n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        critic1_loss.backward()\n",
    "        self.critic1_optimizer.step()\n",
    "\n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        critic2_loss.backward()\n",
    "        self.critic2_optimizer.step()\n",
    "\n",
    "        # Update actor network\n",
    "        sampled_actions = self.actor(state_batch)\n",
    "        actor_loss = -self.critic1(state_batch, sampled_actions).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update target networks\n",
    "        for param, target_param in zip(self.critic1.parameters(), self.critic1_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        for param, target_param in zip(self.critic2.parameters(), self.critic2_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        return critic1_loss.item()+critic2_loss.item()+actor_loss.item()"
   ],
   "metadata": {
    "id": "eFj7uZWvJc3P"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n",
    "# Define hyperparameters\n",
    "state_dim = 6\n",
    "action_dim = 2\n",
    "hidden_dim = 256\n",
    "alpha = 0.2\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "capacity = 100000\n",
    "batch_size = 64\n",
    "max_episodes = 5000\n",
    "max_steps_per_episode = 1000\n",
    "env=gym.make(\"parking-v0\", render_mode=\"rgb_array\")\n",
    "env.unwrapped.config['add_walls']=False\n",
    "env.unwrapped.config['duration']=40\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lengths_list=[]\n",
    "rewards_list=[]\n",
    "rendered_frames=[]\n",
    "losses_list=[]\n",
    "# Initialize SAC agent\n",
    "agent = SAC(state_dim, action_dim, hidden_dim, alpha, gamma, tau)\n",
    "\n",
    "# Initialize replay buffer\n",
    "replay_buffer = ReplayBuffer(capacity)\n",
    "pbar = tqdm.trange(max_episodes)\n",
    "\n",
    "# Training loop\n",
    "for episode in pbar:\n",
    "    state,_ = env.reset(seed=0)\n",
    "    state=state['observation']\n",
    "    episode_reward = 0\n",
    "    t=0\n",
    "    for step in range(1000):\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, truncated,_ = env.step(action)\n",
    "        next_state=next_state['observation']\n",
    "        replay_buffer.add(state, action, reward, next_state, done)\n",
    "        episode_reward += reward\n",
    "        t+=1\n",
    "        state = next_state\n",
    "\n",
    "        if episode%499==0:\n",
    "          rendered_frames.append(env.render())\n",
    "\n",
    "        if len(replay_buffer.buffer) > batch_size:\n",
    "            loss=agent.update(replay_buffer, batch_size)\n",
    "\n",
    "        if done or truncated:\n",
    "            pbar.set_description(\n",
    "                f'Episode: {episode} | Steps: {t} | Return: {episode_reward:5.2f} |Loss: {loss}  '\n",
    "      )\n",
    "            lengths_list.append(t)\n",
    "            rewards_list.append(episode_reward)\n",
    "            losses_list.append(loss)\n",
    "            break"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2K4g7fNWFdA",
    "outputId": "b047b7e7-ea45-4679-fdf0-f57437d9c2ad"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def moving_average(data, *, window_size = 50):\n",
    "    \"\"\"Smooths 1-D data array using a moving average.\n",
    "\n",
    "    Args:\n",
    "        data: 1-D numpy.array\n",
    "        window_size: Size of the smoothing window\n",
    "\n",
    "    Returns:\n",
    "        smooth_data: A 1-d numpy.array with the same size as data\n",
    "    \"\"\"\n",
    "    kernel = np.ones(window_size)\n",
    "    smooth_data = np.convolve(data, kernel) / np.convolve(\n",
    "        np.ones_like(data), kernel\n",
    "    )\n",
    "    return smooth_data[: -window_size + 1]"
   ],
   "metadata": {
    "id": "_ww-actPbrul"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# YOUR PLOTTING CODE HERE\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rewards_list, label='Returns (Raw Data)', alpha=0.5)\n",
    "plt.plot(moving_average(rewards_list), label='Returns (Moving Average)', color='orange')\n",
    "plt.title('Returns')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Return')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lengths_list, label='Lengths (Raw Data)', alpha=0.5)\n",
    "plt.plot(moving_average(lengths_list), label='Lengths (Moving Average)', color='orange')\n",
    "plt.title('Lengths')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Length')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses_list, label='Losses (Raw Data)')\n",
    "plt.plot(moving_average(losses_list), label='Losses (Moving Average)', color='orange')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "M6OiI9cjWJG9",
    "outputId": "2e9eb614-8c8c-4829-834e-e0eebf4075d6"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
