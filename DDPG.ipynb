{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3bIKYPiapC8",
    "outputId": "559f2deb-f7b1-44bd-8f80-088eb3e1a880"
   },
   "source": [
    "!pip install gym[atari]\n",
    "!pip install autorom[accept-rom-license]\n",
    "!pip install highway-env\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "x9kLYGx3fBRj",
    "ExecuteTime": {
     "end_time": "2024-12-10T20:51:06.089166Z",
     "start_time": "2024-12-10T20:51:04.841957Z"
    }
   },
   "source": [
    "import highway_env\n",
    "import gymnasium as gym\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import os\n",
    "import cv2\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uDdvr4kScILA",
    "ExecuteTime": {
     "end_time": "2024-12-10T20:51:13.697638Z",
     "start_time": "2024-12-10T20:51:13.683670Z"
    }
   },
   "source": "env = gym.make(\"parking-v0\", render_mode=\"rgb_array\")",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FmYgwRAiaWaF",
    "ExecuteTime": {
     "end_time": "2024-12-10T20:51:19.076579Z",
     "start_time": "2024-12-10T20:51:16.686571Z"
    }
   },
   "source": [
    "env.reset(seed=0)\n",
    "rendered_frames = []\n",
    "\n",
    "# Perform 100 random steps in the environment and store rendered frames\n",
    "for _ in range(100):\n",
    "    action = env.action_space.sample()  # Sample a random action\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    rendered_frames.append(env.render())\n",
    "\n",
    "    if done or truncated==True:\n",
    "      break\n",
    "# Close the environment\n",
    "env.close()"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2MBJ_k7HpOSs",
    "outputId": "cfc7150a-b023-4cd1-ecae-04b3d72980c0"
   },
   "source": [
    "obs,_=env.reset(seed=0)\n",
    "\n",
    "obs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1Pu6l3OiHRFI",
    "ExecuteTime": {
     "end_time": "2024-12-10T20:51:48.061142Z",
     "start_time": "2024-12-10T20:51:48.056279Z"
    }
   },
   "source": [
    "# Actor Model\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim, 512)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.layer3 = nn.Linear(256, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.layer1(state))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.max_action * torch.tanh(self.layer3(x))\n",
    "        return x\n",
    "\n",
    "# Critic Model\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim + action_dim, 512)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.layer3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SCRt6c8AZ1zW",
    "ExecuteTime": {
     "end_time": "2024-12-10T20:51:49.102014Z",
     "start_time": "2024-12-10T20:51:49.097527Z"
    }
   },
   "source": [
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "M1r8bh4BNi0H",
    "ExecuteTime": {
     "end_time": "2024-12-10T20:51:49.967158Z",
     "start_time": "2024-12-10T20:51:49.962887Z"
    }
   },
   "source": [
    "# Ornstein-Uhlenbeck Noise for exploration\n",
    "class OUNoise:\n",
    "    def __init__(self, action_dimension, scale=0.1, mu=0, theta=0.15, sigma=0.2):\n",
    "        self.action_dimension = action_dimension\n",
    "        self.scale = scale\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(action_dimension) * mu\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dimension) * self.mu\n",
    "\n",
    "    def evolve_state(self):\n",
    "        dx = self.theta * (self.mu - self.state) + self.sigma * np.random.randn(len(self.state))\n",
    "        self.state = self.state + dx\n",
    "        return self.state\n",
    "\n",
    "    def get_action(self, action):\n",
    "        return self.scale * self.evolve_state() + action"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "72DkD525Z8_J",
    "ExecuteTime": {
     "end_time": "2024-12-10T20:51:51.001503Z",
     "start_time": "2024-12-10T20:51:50.994048Z"
    }
   },
   "source": [
    "# DDPG Agent\n",
    "class DDPGAgent:\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters())\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.critic_target = Critic(state_dim, action_dim)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters())\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(capacity=100000)\n",
    "        self.ounoise = OUNoise(action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1))\n",
    "        action = self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "        return self.ounoise.get_action(action)\n",
    "\n",
    "    def train(self, batch_size=64):\n",
    "        critic_loss_val=0\n",
    "        actor_loss_val=0\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return 0,0\n",
    "\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
    "        state = torch.FloatTensor(state)\n",
    "        action = torch.FloatTensor(action)\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(1)\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        done = torch.FloatTensor(done).unsqueeze(1)\n",
    "\n",
    "        # Compute the target Q value\n",
    "        target_actions = self.actor_target(next_state)\n",
    "        target_Q = self.critic_target(next_state, target_actions)\n",
    "        target_Q = reward + (0.99 * target_Q * (1 - done))\n",
    "\n",
    "        # Get current Q estimate\n",
    "        current_Q = self.critic(state, action)\n",
    "\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(current_Q, target_Q.detach())\n",
    "\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Compute actor loss\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "\n",
    "        # Optimize the actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # Update the frozen target models\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(param.data * 0.005 + target_param.data * (1 - 0.005))\n",
    "\n",
    "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "            target_param.data.copy_(param.data * 0.005 + target_param.data * (1 - 0.005))\n",
    "\n",
    "        return critic_loss.item(),actor_loss.item()"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSKGpaO8aDoj",
    "outputId": "3ec7b0c6-4f6c-449a-a85b-918986a47a1a",
    "ExecuteTime": {
     "end_time": "2024-12-10T21:02:06.232090638Z",
     "start_time": "2024-12-10T20:51:59.548931Z"
    }
   },
   "source": [
    "# Prepare the environment\n",
    "env=gym.make(\"parking-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "\n",
    "env.unwrapped.config['add_walls']=False\n",
    "env.unwrapped.config['duration']=40\n",
    "state_dim = 6\n",
    "action_dim = 2\n",
    "max_action = float(env.action_space.high[0])\n",
    "rendered_frames=[]\n",
    "agent = DDPGAgent(state_dim, action_dim, max_action)\n",
    "num_steps = 5000\n",
    "i_episode=0\n",
    "rewards=[]\n",
    "lengths=[]\n",
    "losses=[]\n",
    "pbar = tqdm.trange(num_steps)\n",
    "for t_total in pbar:\n",
    "    state,_ = env.reset(seed=0)\n",
    "    state=state['observation']\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    truncated= False\n",
    "    step=0\n",
    "    critic_loss_sum=0\n",
    "    actor_loss_sum=0\n",
    "    training_steps=0\n",
    "\n",
    "    for _ in range(1000):\n",
    "        step+=1\n",
    "        action = agent.select_action(np.array(state))\n",
    "        next_state, reward, done, truncated ,info = env.step(action)\n",
    "        next_state=next_state['observation']\n",
    "\n",
    "\n",
    "        agent.replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "        critic_loss,actor_loss=agent.train(batch_size=64)\n",
    "        critic_loss_sum += critic_loss\n",
    "        actor_loss_sum += actor_loss\n",
    "        if critic_loss and actor_loss:\n",
    "            training_steps +=1\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if t_total%499 == 0:\n",
    "          rendered_frames.append(env.render())\n",
    "\n",
    "\n",
    "        if done or truncated:\n",
    "          avg_critic_loss =critic_loss_sum/training_steps if training_steps else 0\n",
    "          avg_actor_loss=actor_loss_sum/training_steps if training_steps else 0\n",
    "          total_loss=avg_critic_loss+avg_critic_loss\n",
    "          pbar.set_description(\n",
    "                      f'Episode: {i_episode} | Steps: {step + 1} | Return: {episode_reward:5.2f} |Loss: {total_loss} '\n",
    "            )\n",
    "          lengths.append(step+1)\n",
    "          losses.append(total_loss)\n",
    "          rewards.append(episode_reward)\n",
    "          break\n",
    "    i_episode+=1\n",
    "env.close()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode: 146 | Steps: 201 | Return: -80.63 |Loss: 0.0817616552952677 :   3%|â–Ž         | 147/5000 [10:04<5:21:48,  3.98s/it]  "
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aXVbAp4Bbc9C"
   },
   "outputs": [],
   "source": [
    "def moving_average(data, *, window_size = 50):\n",
    "    \"\"\"Smooths 1-D data array using a moving average.\n",
    "\n",
    "    Args:\n",
    "        data: 1-D numpy.array\n",
    "        window_size: Size of the smoothing window\n",
    "\n",
    "    Returns:\n",
    "        smooth_data: A 1-d numpy.array with the same size as data\n",
    "    \"\"\"\n",
    "    kernel = np.ones(window_size)\n",
    "    smooth_data = np.convolve(data, kernel) / np.convolve(\n",
    "        np.ones_like(data), kernel\n",
    "    )\n",
    "    return smooth_data[: -window_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "file_path_DDPG_returns = \"DDPG_Returns.pkl\"\n",
    "file_path_DDPG_losses=\"DDPG_Losses.pkl\"\n",
    "file_path_DDPG_frames=\"DDPG_frames.pkl\""
   ],
   "metadata": {
    "id": "Q5rh5W3Ep3yY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with open(file_path_DDPG_returns, 'wb') as f:\n",
    "    pickle.dump(rewards, f)\n",
    "with open(file_path_DDPG_losses, 'wb') as f:\n",
    "    pickle.dump(losses, f)\n",
    "with open(file_path_DDPG_frames, 'wb') as f:\n",
    "    pickle.dump(rendered_frames, f)"
   ],
   "metadata": {
    "id": "m-LvRKDaq7oP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with open(file_path_DDPG_returns, 'rb') as f :\n",
    "  DDPG_returns = pickle.load(f)\n",
    "with open(file_path_DDPG_losses, 'rb') as f :\n",
    "  DDPG_losses = pickle.load(f)\n",
    "with open(file_path_DDPG_frames, 'rb') as f :\n",
    "  DDPG_frames = pickle.load(f)"
   ],
   "metadata": {
    "id": "yPSNQBccr6Ex"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C8ENBX5nbVdP",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "71bdc204-2315-4141-a106-b23904844d02"
   },
   "source": [
    "# YOUR PLOTTING CODE HERE\n",
    "plt.figure(figsize=(10, 6))\n",
    "# plt.subplot(3, 1, 1)\n",
    "plt.plot(DDPG_returns, label='Returns (Raw Data)', alpha=0.5)\n",
    "plt.plot(moving_average(DDPG_returns), label='Returns (Moving Average)', color='orange')\n",
    "plt.title('Returns')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Return')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lengths, label='Lengths (Raw Data)', alpha=0.5)\n",
    "plt.plot(moving_average(lengths), label='Lengths (Moving Average)', color='orange')\n",
    "plt.title('Lengths')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Length')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(DDPG_losses, label='Losses (Raw Data)')\n",
    "plt.plot(moving_average(DDPG_losses), label='Losses (Moving Average)', color='orange')\n",
    "plt.title('Losses')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
